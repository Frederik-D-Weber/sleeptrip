
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>examples</title><meta name="generator" content="MATLAB 7.13"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2012-05-11"><meta name="DC.source" content="examples.m"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head><body><div class="content"><h2>Contents</h2><div><ul><li><a href="#1">Examples</a></li><li><a href="#2">Elastic net regression</a></li></ul></div><h2>Examples<a name="1"></a></h2><p>In the following examples we will make use of a neuroimaging dataset acquired at the Donders Institute. This data and a description can be downloaded from <a href="http://www.distrep.org/data">http://www.distrep.org/data</a></p><pre class="codeinput">load <span class="string">69digits</span>
</pre><h2>Elastic net regression<a name="2"></a></h2><p>Elastic net regression can be used to get sparse solutions where regression coefficient vectors consist of many zeros. Elastic net regression allows for linear and logistic regression depending on the 'family' parameter ('gaussian' or 'binomial'). Here, we deal with logistic regression only and focus on classification. This toolbox implements two versions of elastic net: dml.graphnet and dml.enet. The former is more general than the standard elastic net and implemented in Matlab. The latter is less general and calls external Fortran code, making it much faster. In the following, we show how to use these two implementations on our example data.</p><p>dml.graphnet minimizes the sum squared error on the data plus a regularization term of the form:</p><p><img src="examples_eq62684.png" alt="$$L_1 ||\beta||_1 + \frac{1}{2} \beta' L_2 \beta$$"></p><p>where the betas are the regression coefficients, L1 is a scalar and L2 is a scalar or a matrix.  The L1 and L2 parameters provide a different form of regularization, with L1 giving sparse solutions and L2 giving smooth solutions. For the moment, we fix L2 to a small constant (default is 1e-6) and want to find the solution for L1=0.1.</p><pre class="codeinput">m = dml.graphnet(<span class="string">'family'</span>,<span class="string">'binomial'</span>,<span class="string">'L1'</span>,0.1);
m = m.train(X,Y);
</pre><p>The sparse solution is returned in the model.weights field:</p><pre class="codeinput">bar(m.model.weights);
</pre><img vspace="5" hspace="5" src="examples_01.png" alt=""> <p>Note however that directly finding the solution for a small L1 value can be numerically unstable. It's better to approximate this solution using small decreases in the L1 value. Furthermore, typically, the value of L1 is unknown and one wants to find such a value. This is where the gridsearch comes into play. We first generate a suitable regularization path (values of L1) to follow using a helper function</p><pre class="codeinput">v = dml.graphnet.lambdapath(X,Y,<span class="string">'binomial'</span>,50,1e-2);
</pre><p>and then we use it in a gridsearch</p><pre class="codeinput">m = dml.gridsearch(<span class="string">'validator'</span>,dml.crossvalidator(<span class="string">'type'</span>,<span class="string">'split'</span>,<span class="string">'stat'</span>,<span class="string">'accuracy'</span>,<span class="string">'mva'</span>,dml.graphnet(<span class="string">'family'</span>,<span class="string">'binomial'</span>,<span class="string">'restart'</span>,false)),<span class="string">'vars'</span>,<span class="string">'L1'</span>,<span class="string">'vals'</span>,v);
tic; m = m.train(X,Y); toc
</pre><pre class="codeoutput">Elapsed time is 12.409168 seconds.
</pre><p>In order to see how performance changes as a function of L1, we can use the following code:</p><pre class="codeinput">plot(m.configs,m.outcome); xlabel(<span class="string">'L1'</span>); ylabel(<span class="string">'accuracy'</span>)
</pre><img vspace="5" hspace="5" src="examples_02.png" alt=""> <p>The associated configuration of parameter values is used to retrain the model on all data. This model can be accessed using the m.model field. Note that this model can differ from the model(s) obtained using crossvalidation since all data is used:</p><pre class="codeinput">subplot(1,2,1); bar(m.models{m.optimum}.weights);
subplot(1,2,2); bar(m.model.weights);
</pre><img vspace="5" hspace="5" src="examples_03.png" alt=""> <p>The same results can be achieved using dml.glmnet. However, here the regularization term is a bit different:</p><p><img src="examples_eq33877.png" alt="$$\lambda( (1-\alpha) \frac{1}{2}||\beta||^2_2 + \alpha ||\beta||_1 )$$"></p><p>Furthermore, the cross-validation and determination of the regularization path is done automatically inside the model. We can call this code as follows:</p><pre class="codeinput">m = dml.enet;
tic; m = m.train(X,Y); toc;
close; bar(m.model.weights);
</pre><pre class="codeoutput">Elapsed time is 0.926965 seconds.
</pre><img vspace="5" hspace="5" src="examples_04.png" alt=""> <p>We can plot the contributions to the decoding back to brain space as follows:</p><pre class="codeinput">M=mask; M(M) = m.model.weights;
imagesc(1e4*mean(abs(M),3)+mean(structural,3)); axis <span class="string">off</span>; axis <span class="string">square</span>;
</pre><img vspace="5" hspace="5" src="examples_05.png" alt=""> <p class="footer"><br>
      Published with MATLAB&reg; 7.13<br></p></div><!--
##### SOURCE BEGIN #####
%% Examples
% In the following examples we will make use of a neuroimaging dataset
% acquired at the Donders Institute. This data and a description can be
% downloaded from http://www.distrep.org/data
load 69digits

%% Elastic net regression
% Elastic net regression can be used to get sparse solutions where
% regression coefficient vectors consist of many zeros. Elastic net
% regression allows for linear and logistic regression depending on the
% 'family' parameter ('gaussian' or 'binomial'). Here, we deal with
% logistic regression only and focus on classification. This toolbox
% implements two versions of elastic net: dml.graphnet and dml.enet. The
% former is more general than the standard elastic net and implemented in
% Matlab. The latter is less general and calls external Fortran code,
% making it much faster. In the following, we show how to use these two
% implementations on our example data.
%
% dml.graphnet minimizes the sum squared error on the data plus a
% regularization term of the form:
%
% $$L_1 ||\beta||_1 + \frac{1}{2} \beta' L_2 \beta$$
%
% where the betas are the regression coefficients, L1 is a scalar and L2 is
% a scalar or a matrix.  The L1 and L2 parameters provide a different form
% of regularization, with L1 giving sparse solutions and L2 giving smooth
% solutions. For the moment, we fix L2 to a small constant (default is
% 1e-6) and want to find the solution for L1=0.1.
m = dml.graphnet('family','binomial','L1',0.1);
m = m.train(X,Y);
%% 
% The sparse solution is returned in the model.weights field:
bar(m.model.weights); 
%%
% Note however that directly finding the solution for a small L1 value can
% be numerically unstable. It's better to approximate this solution using
% small decreases in the L1 value. Furthermore, typically, the value of L1
% is unknown and one wants to find such a value. This is where the
% gridsearch comes into play. We first generate a suitable regularization
% path (values of L1) to follow using a helper function
v = dml.graphnet.lambdapath(X,Y,'binomial',50,1e-2);
%%
% and then we use it in a gridsearch
m = dml.gridsearch('validator',dml.crossvalidator('type','split','stat','accuracy','mva',dml.graphnet('family','binomial','restart',false)),'vars','L1','vals',v);
tic; m = m.train(X,Y); toc
%%
% In order to see how performance changes as a function of L1, we can use
% the following code:
plot(m.configs,m.outcome); xlabel('L1'); ylabel('accuracy')
%%
% The associated configuration of parameter values is used to
% retrain the model on all data. This model can be accessed using the
% m.model field. Note that this model can differ from the model(s) obtained
% using crossvalidation since all data is used:
subplot(1,2,1); bar(m.models{m.optimum}.weights);
subplot(1,2,2); bar(m.model.weights);

%%
% The same results can be achieved using dml.glmnet. However, here the regularization
% term is a bit different:
%
% $$\lambda( (1-\alpha) \frac{1}{2}||\beta||^2_2 + \alpha ||\beta||_1 )$$ 
%
% Furthermore, the cross-validation and determination of the regularization 
% path is done automatically inside the model. We can call this code as follows:
m = dml.enet;
tic; m = m.train(X,Y); toc;
close; bar(m.model.weights);
%%
% We can plot the contributions to the decoding back to brain space as follows:
M=mask; M(M) = m.model.weights;
imagesc(1e4*mean(abs(M),3)+mean(structural,3)); axis off; axis square;

##### SOURCE END #####
--></body></html>